{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MS-SNSD VAD Classifier for Clean Speech vs. Noise\n",
        "--------------------------------------------------\n",
        "Dataset details:\n",
        "- CleanSpeech: clean speech files (16 kHz, 16-bit WAV)\n",
        "- Noise: noise files (16 kHz, 16-bit WAV)\n",
        "The task is to classify entire audio files as either speech (label 1) or noise (label 0).\n",
        "\n",
        "This script builds a robust CNN classifier using MFCC spectrograms as input.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ========================\n",
        "# Audio Loading and MFCC Extraction Functions\n",
        "# ========================\n",
        "\n",
        "def load_audio(file_path, sample_rate=16000):\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        if sr != sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
        "            waveform = resampler(waveform)\n",
        "        return waveform.numpy().flatten()\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "def extract_mfcc_spectrogram(audio, sr=16000, n_mfcc=40, n_fft=512, hop_length=160):\n",
        "    \"\"\"\n",
        "    Compute an MFCC spectrogram.\n",
        "    Parameters:\n",
        "      - n_mfcc: number of MFCC coefficients (using 40 here for better representation)\n",
        "      - n_fft: FFT window length (set to the minimum of n_fft and len(audio))\n",
        "      - hop_length: hop length between frames\n",
        "    Returns a numpy array of shape (n_mfcc, time_frames).\n",
        "    \"\"\"\n",
        "    n_fft = min(n_fft, len(audio))\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "    return mfccs\n",
        "\n",
        "# ========================\n",
        "# Custom Collate Function to Pad Variable Length MFCC Spectrograms\n",
        "# ========================\n",
        "def pad_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Each sample in the batch is a tuple (features, label) where features is a 2D numpy array\n",
        "    of shape (n_mfcc, frames). We pad along the time axis so that all samples have the same number of frames.\n",
        "    Finally, we add a channel dimension for CNN input.\n",
        "    \"\"\"\n",
        "    # Filter out any None samples (if any)\n",
        "    batch = [sample for sample in batch if sample is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    features, labels = zip(*batch)\n",
        "    # Convert features to tensors and get maximum time frames\n",
        "    feature_tensors = [torch.tensor(f, dtype=torch.float32) for f in features]\n",
        "    max_frames = max(f.shape[1] for f in feature_tensors)\n",
        "    # Pad each sample (pad along time dimension, i.e. dimension 1)\n",
        "    padded_features = []\n",
        "    for f in feature_tensors:\n",
        "        pad_amount = max_frames - f.shape[1]\n",
        "        if pad_amount > 0:\n",
        "            # Pad on the right side along the time dimension\n",
        "            f = nn.functional.pad(f, (0, pad_amount), \"constant\", 0)\n",
        "        padded_features.append(f)\n",
        "    # Stack into a tensor and add channel dimension: shape (batch_size, 1, n_mfcc, max_frames)\n",
        "    batch_features = torch.stack(padded_features).unsqueeze(1)\n",
        "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return batch_features, batch_labels\n",
        "\n",
        "# ========================\n",
        "# Dataset Classes\n",
        "# ========================\n",
        "\n",
        "class CleanSpeechDataset(Dataset):\n",
        "    def __init__(self, speech_dir, sample_rate=16000):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.file_paths = []\n",
        "        self.labels = []  # Label 1 for speech\n",
        "        # Recursively find all WAV (or FLAC) files\n",
        "        for root, dirs, files in os.walk(speech_dir):\n",
        "            for f in files:\n",
        "                if f.lower().endswith('.wav') or f.lower().endswith('.flac'):\n",
        "                    self.file_paths.append(os.path.join(root, f))\n",
        "                    self.labels.append(1)\n",
        "        print(f\"CleanSpeechDataset: Found {len(self.file_paths)} speech files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = load_audio(self.file_paths[idx], self.sample_rate)\n",
        "        if audio.size == 0:\n",
        "            raise ValueError(f\"Failed to load {self.file_paths[idx]}\")\n",
        "        # Compute MFCC spectrogram (without averaging)\n",
        "        mfcc_spec = extract_mfcc_spectrogram(audio, sr=self.sample_rate)\n",
        "        label = self.labels[idx]\n",
        "        return mfcc_spec, label\n",
        "\n",
        "class NoiseDataset(Dataset):\n",
        "    def __init__(self, noise_dir, sample_rate=16000):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.file_paths = [os.path.join(noise_dir, f) for f in os.listdir(noise_dir) if f.lower().endswith('.wav')]\n",
        "        self.labels = [0] * len(self.file_paths)  # Label 0 for noise\n",
        "        print(f\"NoiseDataset: Found {len(self.file_paths)} noise files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = load_audio(self.file_paths[idx], self.sample_rate)\n",
        "        if audio.size == 0:\n",
        "            raise ValueError(f\"Failed to load {self.file_paths[idx]}\")\n",
        "        mfcc_spec = extract_mfcc_spectrogram(audio, sr=self.sample_rate)\n",
        "        label = self.labels[idx]\n",
        "        return mfcc_spec, label\n",
        "\n",
        "# ========================\n",
        "# CNN Classifier Definition\n",
        "# ========================\n",
        "\n",
        "class VAD_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAD_CNN, self).__init__()\n",
        "        # Input shape: (batch, 1, n_mfcc, time_frames). Here we use n_mfcc=40.\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Global average pooling: Adaptive pooling to (1, 1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)  # 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 1, 40, time_frames)\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.global_pool(x)  # shape: (batch, 64, 1, 1)\n",
        "        x = x.view(x.size(0), -1)  # shape: (batch, 64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ========================\n",
        "# Training and Evaluation Function\n",
        "# ========================\n",
        "\n",
        "def train_and_evaluate(clean_speech_dir, noise_dir):\n",
        "    # Create datasets for clean speech and noise\n",
        "    speech_ds = CleanSpeechDataset(clean_speech_dir)\n",
        "    noise_ds = NoiseDataset(noise_dir)\n",
        "    # Merge datasets\n",
        "    combined_ds = ConcatDataset([speech_ds, noise_ds])\n",
        "    print(f\"Combined dataset contains {len(combined_ds)} samples.\")\n",
        "    \n",
        "    # Split into training and testing sets (80-20 split)\n",
        "    all_samples = list(combined_ds)\n",
        "    train_samples, test_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Create DataLoaders with custom collate function for padding\n",
        "    train_loader = DataLoader(train_samples, batch_size=32, shuffle=True, collate_fn=pad_collate_fn)\n",
        "    test_loader = DataLoader(test_samples, batch_size=32, shuffle=False, collate_fn=pad_collate_fn)\n",
        "    \n",
        "    # Initialize CNN model, loss function, optimizer\n",
        "    model = VAD_CNN().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    num_epochs = 30\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * features.size(0)\n",
        "        epoch_loss = running_loss / len(train_samples)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "    \n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    \n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    \n",
        "    # Plot metrics\n",
        "    metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "    metrics_values = [acc, prec, rec, f1]\n",
        "    x = np.arange(len(metrics_labels))\n",
        "    width = 0.35\n",
        "    fig, ax = plt.subplots()\n",
        "    rects = ax.bar(x, metrics_values, width, label=\"Metrics\")\n",
        "    ax.set_ylabel(\"Scores\")\n",
        "    ax.set_title(\"VAD Classifier Performance on MS-SNSD\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics_labels)\n",
        "    ax.legend()\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f\"{height:.4f}\",\n",
        "                        xy=(rect.get_x()+rect.get_width()/2, height),\n",
        "                        xytext=(0,3), textcoords=\"offset points\",\n",
        "                        ha=\"center\", va=\"bottom\")\n",
        "    autolabel(rects)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ========================\n",
        "# Run the training and evaluation\n",
        "# ========================\n",
        "\n",
        "# Update these paths to your local directories for MS-SNSD\n",
        "clean_speech_dir = \"./CleanSpeech\"  # Directory containing clean speech files\n",
        "noise_dir = \"./Noise\"               # Directory containing noise files\n",
        "\n",
        "train_and_evaluate(clean_speech_dir, noise_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
